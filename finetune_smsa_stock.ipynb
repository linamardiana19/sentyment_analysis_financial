{
 "cells": [
  {
   "cell_type": "raw",
   "id": "aaa0a508-f635-420d-a799-0b5cb8fb3335",
   "metadata": {},
   "source": [
    "# copy right by Lina Mardiana Harahap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71f90f12-9356-438f-bb30-80ff5d9f4da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append('../')\n",
    "os.chdir('../')\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "# from transformers import AutoTokenizer, AutoConfig, AutoModelForSequenceClassification, BertTokenizer\n",
    "# from nltk.tokenize import TweetTokenizer\n",
    "from transformers import BertTokenizer, BertConfig, BertForSequenceClassification\n",
    "\n",
    "from utils.forward_fn import forward_sequence_classification\n",
    "from utils.metrics import document_sentiment_metrics_fn\n",
    "# from utils.data_utils_kazee3 import DocumentSentimentDataset, DocumentSentimentDataLoader\n",
    "from utils.data_utils import DocumentSentimentDataset, DocumentSentimentDataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e221bc2f-f001-44e9-8810-8c2eed314266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is cuda available? True\n",
      "Device count? 4\n",
      "Current device? 0\n",
      "Device name?  NVIDIA RTX A5000\n"
     ]
    }
   ],
   "source": [
    "print(\"Is cuda available?\", torch.cuda.is_available())\n",
    "print(\"Device count?\", torch.cuda.device_count())\n",
    "print(\"Current device?\", torch.cuda.current_device())\n",
    "print(\"Device name? \", torch.cuda.get_device_name(torch.cuda.current_device()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1856e8e1-e8d2-4f5e-9436-17b58ccc2ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7ff7933-68ab-4b9c-8f7f-25c652f7b386",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "\n",
    "def count_param(module, trainable=False):\n",
    "    if trainable:\n",
    "        return sum(p.numel() for p in module.parameters() if p.requires_grad)\n",
    "    else:\n",
    "        return sum(p.numel() for p in module.parameters())\n",
    "\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "\n",
    "\n",
    "def metrics_to_string(metric_dict):\n",
    "    string_list = []\n",
    "    for key, value in metric_dict.items():\n",
    "        string_list.append('{}:{:.2f}'.format(key, value))\n",
    "    return ' '.join(string_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f7b532b-ba76-4d13-adc6-48148a0fa3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(25072024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec49a96e-a4ab-473e-9a11-c91d6716b600",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "290181df-7690-4731-84dc-d80361d7c032",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load Tokenizer and Config\n",
    "tokenizer = BertTokenizer.from_pretrained('indobenchmark/indobert-base-p1')\n",
    "config = BertConfig.from_pretrained('indobenchmark/indobert-base-p1')\n",
    "config.num_labels = DocumentSentimentDataset.NUM_LABELS\n",
    "\n",
    "# Instantiate model\n",
    "model = BertForSequenceClassification.from_pretrained('indobenchmark/indobert-base-p1', config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59535103-f58f-4187-9813-6da7757a6886",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(50000, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f8ed7e-bf78-41ae-9afc-1d44cfc51178",
   "metadata": {},
   "source": [
    "# Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3753ca46-e1c1-42d6-9172-2e0cc3e55c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save = './dataset/dataset_smsa_stok/save_model/smsa_save_model'\n",
    "\n",
    "train_dataset_path = './dataset/dataset_smsa_stok/data_clean/data_train/data_smsa_train.tsv'\n",
    "valid_dataset_path = './dataset/dataset_smsa_stok/data_clean/data_train/data_smsa_valid.tsv'\n",
    "# test_dataset_path = './dataset/smsa_doc-sentiment-prosa/test_preprocess_masked_label.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7985b046-79b2-46c7-9b73-02a3aa818023",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DocumentSentimentDataset(train_dataset_path, tokenizer, lowercase=True)\n",
    "valid_dataset = DocumentSentimentDataset(valid_dataset_path, tokenizer, lowercase=True)\n",
    "# test_dataset = DocumentSentimentDataset(test_dataset_path, tokenizer, lowercase=True)\n",
    "\n",
    "train_loader = DocumentSentimentDataLoader(dataset=train_dataset, max_seq_len=512, batch_size=32, num_workers=16, shuffle=True)  \n",
    "valid_loader = DocumentSentimentDataLoader(dataset=valid_dataset, max_seq_len=512, batch_size=32, num_workers=16, shuffle=False)  \n",
    "# test_loader = DocumentSentimentDataLoader(dataset=test_dataset, max_seq_len=512, batch_size=32, num_workers=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ea46502-ba2e-4789-82b6-48ad1f67f9f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'positive': 0, 'neutral': 1, 'negative': 2}\n",
      "{0: 'positive', 1: 'neutral', 2: 'negative'}\n"
     ]
    }
   ],
   "source": [
    "w2i, i2w = DocumentSentimentDataset.LABEL2INDEX, DocumentSentimentDataset.INDEX2LABEL\n",
    "print(w2i)\n",
    "print(i2w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e714d687-94f8-4549-86ea-d1bf4b92ced5",
   "metadata": {},
   "source": [
    "# Test model on sample sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10576dd2-afc1-4d38-8b19-60834a8face7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: btw saham bbca dan bank mandiri on fire ya sejak januari demikian jg saham bbri dan bbni | Label : positive (37.952%)\n"
     ]
    }
   ],
   "source": [
    "text = 'btw saham bbca dan bank mandiri on fire ya sejak januari demikian jg saham bbri dan bbni'\n",
    "subwords = tokenizer.encode(text)\n",
    "subwords = torch.LongTensor(subwords).view(1, -1).to(model.device)\n",
    "\n",
    "logits = model(subwords)[0]\n",
    "label = torch.topk(logits, k=1, dim=-1)[1].squeeze().item()\n",
    "\n",
    "print(f'Text: {text} | Label : {i2w[label]} ({F.softmax(logits, dim=-1).squeeze()[label] * 100:.3f}%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402f6e4c-7866-46b9-985d-0828d28a3807",
   "metadata": {},
   "source": [
    "# Fine Tuning & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "365e67cb-8610-4ea0-8251-3a74dc4e703f",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=3e-6)\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4c6733d9-2f00-4ee3-a68a-0dc16eb90c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 1) TRAIN LOSS:0.2272 LR:0.00000300: 100%|██████████| 83/83 [00:15<00:00,  5.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 1) TRAIN LOSS:0.2272 ACC:0.93 F1:0.92 REC:0.91 PRE:0.92 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5020 ACC:0.81 F1:0.80 REC:0.78 PRE:0.82: 100%|██████████| 21/21 [00:03<00:00,  5.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 1) VALID LOSS:0.5020 ACC:0.81 F1:0.80 REC:0.78 PRE:0.82\n",
      "current best\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.80      0.91      0.85       340\n",
      "     neutral       0.83      0.70      0.76       158\n",
      "    negative       0.83      0.73      0.78       160\n",
      "\n",
      "    accuracy                           0.81       658\n",
      "   macro avg       0.82      0.78      0.80       658\n",
      "weighted avg       0.82      0.81      0.81       658\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 2) TRAIN LOSS:0.1630 LR:0.00000300: 100%|██████████| 83/83 [00:15<00:00,  5.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 2) TRAIN LOSS:0.1630 ACC:0.95 F1:0.95 REC:0.95 PRE:0.95 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5047 ACC:0.82 F1:0.81 REC:0.81 PRE:0.81: 100%|██████████| 21/21 [00:03<00:00,  5.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 2) VALID LOSS:0.5047 ACC:0.82 F1:0.81 REC:0.81 PRE:0.81\n",
      "current best\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.87      0.85      0.86       340\n",
      "     neutral       0.79      0.77      0.78       158\n",
      "    negative       0.76      0.81      0.79       160\n",
      "\n",
      "    accuracy                           0.82       658\n",
      "   macro avg       0.81      0.81      0.81       658\n",
      "weighted avg       0.82      0.82      0.82       658\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 3) TRAIN LOSS:0.1240 LR:0.00000300: 100%|██████████| 83/83 [00:16<00:00,  5.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 3) TRAIN LOSS:0.1240 ACC:0.96 F1:0.96 REC:0.96 PRE:0.96 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5314 ACC:0.82 F1:0.80 REC:0.81 PRE:0.80: 100%|██████████| 21/21 [00:03<00:00,  6.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 3) VALID LOSS:0.5314 ACC:0.82 F1:0.80 REC:0.81 PRE:0.80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 4) TRAIN LOSS:0.0857 LR:0.00000300: 100%|██████████| 83/83 [00:15<00:00,  5.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 4) TRAIN LOSS:0.0857 ACC:0.98 F1:0.97 REC:0.97 PRE:0.98 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5806 ACC:0.82 F1:0.81 REC:0.82 PRE:0.80: 100%|██████████| 21/21 [00:03<00:00,  5.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 4) VALID LOSS:0.5806 ACC:0.82 F1:0.81 REC:0.82 PRE:0.80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 5) TRAIN LOSS:0.0668 LR:0.00000300: 100%|██████████| 83/83 [00:15<00:00,  5.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 5) TRAIN LOSS:0.0668 ACC:0.98 F1:0.98 REC:0.98 PRE:0.98 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.6063 ACC:0.81 F1:0.80 REC:0.80 PRE:0.81: 100%|██████████| 21/21 [00:03<00:00,  5.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 5) VALID LOSS:0.6063 ACC:0.81 F1:0.80 REC:0.80 PRE:0.81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Fungsi untuk menghitung dan menampilkan classification report\n",
    "def display_classification_report(list_hyp, list_label):\n",
    "    list_hyp_idx = [DocumentSentimentDataset.LABEL2INDEX[hyp] for hyp in list_hyp]\n",
    "    list_label_idx = [DocumentSentimentDataset.LABEL2INDEX[label] for label in list_label]\n",
    "    \n",
    "    target_names = [DocumentSentimentDataset.INDEX2LABEL[i] for i in range(DocumentSentimentDataset.NUM_LABELS)]\n",
    "    \n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(list_label_idx, list_hyp_idx, target_names=target_names))\n",
    "\n",
    "# Train\n",
    "n_epochs = 5\n",
    "best_f1 = 0\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    torch.set_grad_enabled(True)\n",
    "\n",
    "    total_train_loss = 0\n",
    "    list_hyp, list_label = [], []\n",
    "\n",
    "    train_pbar = tqdm(train_loader, leave=True, total=len(train_loader))\n",
    "    for i, batch_data in enumerate(train_pbar):\n",
    "        # Forward model\n",
    "        loss, batch_hyp, batch_label = forward_sequence_classification(model, batch_data[:-1], i2w=i2w, device='cuda')\n",
    "\n",
    "        # Update model\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        tr_loss = loss.item()\n",
    "        total_train_loss += tr_loss\n",
    "\n",
    "        # Calculate metrics\n",
    "        list_hyp += batch_hyp\n",
    "        list_label += batch_label\n",
    "\n",
    "        train_pbar.set_description(\"(Epoch {}) TRAIN LOSS:{:.4f} LR:{:.8f}\".format((epoch + 1),\n",
    "                                                                                   total_train_loss / (i + 1), get_lr(optimizer)))\n",
    "\n",
    "    # Calculate train metric\n",
    "    metrics = document_sentiment_metrics_fn(list_hyp, list_label)\n",
    "    print(\"(Epoch {}) TRAIN LOSS:{:.4f} {} LR:{:.8f}\".format((epoch + 1),\n",
    "                                                             total_train_loss / (i + 1), metrics_to_string(metrics), get_lr(optimizer)))\n",
    "\n",
    "    # Evaluate on validation\n",
    "    model.eval()\n",
    "    torch.set_grad_enabled(False)\n",
    "\n",
    "    total_loss, total_correct, total_labels = 0, 0, 0\n",
    "    list_hyp, list_label = [], []\n",
    "\n",
    "    pbar = tqdm(valid_loader, leave=True, total=len(valid_loader))\n",
    "    for i, batch_data in enumerate(pbar):\n",
    "        batch_seq = batch_data[-1]\n",
    "        loss, batch_hyp, batch_label = forward_sequence_classification(model, batch_data[:-1], i2w=i2w, device='cuda')\n",
    "\n",
    "        # Calculate total loss\n",
    "        valid_loss = loss.item()\n",
    "        total_loss += valid_loss\n",
    "\n",
    "        # Calculate evaluation metrics\n",
    "        list_hyp += batch_hyp\n",
    "        list_label += batch_label\n",
    "        metrics = document_sentiment_metrics_fn(list_hyp, list_label)\n",
    "\n",
    "        pbar.set_description(\"VALID LOSS:{:.4f} {}\".format(\n",
    "            total_loss / (i + 1), metrics_to_string(metrics)))\n",
    "\n",
    "    metrics = document_sentiment_metrics_fn(list_hyp, list_label)\n",
    "    print(\"(Epoch {}) VALID LOSS:{:.4f} {}\".format((epoch + 1),\n",
    "                                                   total_loss / (i + 1), metrics_to_string(metrics)))\n",
    "\n",
    "    if metrics['F1'] > best_f1:\n",
    "        best_f1 = metrics['F1']\n",
    "        model.save_pretrained(model_save)\n",
    "        tokenizer.save_pretrained(model_save)\n",
    "        config.save_pretrained(model_save)\n",
    "\n",
    "        print('current best')\n",
    "        # Display classification report for the current best model\n",
    "        display_classification_report(list_hyp, list_label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b379ff-1168-4bca-8531-0838e62c7550",
   "metadata": {},
   "source": [
    "## Test Model for Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6b53a3a1-43d3-4138-808e-85f783cb4854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: btw saham bbca dan bank mandiri on fire ya sejak januari demikian jg saham bbri dan bbni | Label : positive (99.022%)\n"
     ]
    }
   ],
   "source": [
    "text = 'btw saham bbca dan bank mandiri on fire ya sejak januari demikian jg saham bbri dan bbni'\n",
    "subwords = tokenizer.encode(text)\n",
    "subwords = torch.LongTensor(subwords).view(1, -1).to(model.device)\n",
    "\n",
    "logits = model(subwords)[0]\n",
    "label = torch.topk(logits, k=1, dim=-1)[1].squeeze().item()\n",
    "\n",
    "print(\n",
    "    f'Text: {text} | Label : {i2w[label]} ({F.softmax(logits, dim=-1).squeeze()[label] * 100:.3f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "869ec8e9-1d9c-4395-82ec-ae9010a472dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: aku gatau yaa ga percaya gitu sama bmri | Label : positive (83.946%)\n"
     ]
    }
   ],
   "source": [
    "text = 'aku gatau yaa ga percaya gitu sama bmri'\n",
    "subwords = tokenizer.encode(text)\n",
    "subwords = torch.LongTensor(subwords).view(1, -1).to(model.device)\n",
    "\n",
    "logits = model(subwords)[0]\n",
    "label = torch.topk(logits, k=1, dim=-1)[1].squeeze().item()\n",
    "\n",
    "print(\n",
    "    f'Text: {text} | Label : {i2w[label]} ({F.softmax(logits, dim=-1).squeeze()[label] * 100:.3f}%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9f4d4c-90ea-4dc1-8086-e404ce97e975",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a131b6a6-fb29-4857-8a0f-0bc69321be4d",
   "metadata": {},
   "source": [
    "Model IndoBERT telah berhasil di-fine-tune menggunakan dataset ID-SMSA: Indonesian Stock Market Dataset for Sentiment Analysis yang dikembangkan oleh Hartanto, Jason; Liundi, Timothy; dan Sutoyo, Rhio (2025), tersedia pada Mendeley Data versi 3 (doi: 10.17632/tn4vzs8tdw.3). Dataset ini terdiri dari data berita pasar saham Indonesia yang telah diberi label sentimen ke dalam tiga kategori: positif, netral, dan negatif. Proses fine-tuning dilakukan untuk menyesuaikan model dengan karakteristik data finansial berbahasa Indonesia.\n",
    "\n",
    "Hasil pelatihan menunjukkan bahwa model mencapai performa terbaik pada epoch ke-2, dengan nilai akurasi validasi sebesar 82%. Evaluasi performa berdasarkan metrik f1-score menunjukkan bahwa model paling baik dalam mengenali sentimen positif, dengan f1-score sebesar 0.86. Untuk kelas netral, model memperoleh f1-score sebesar 0.78 , sedangkan untuk kelas negatif f1-score yang diperoleh adalah 0.79. Secara keseluruhan, f1-score rata-rata (macro average) dari model ini mencapai 0.82, menunjukkan kinerja yang relatif seimbang dalam mengklasifikasikan ketiga jenis sentimen dalam domain berita pasar saham Indonesia."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
